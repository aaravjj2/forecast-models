{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Pipeline Runner for Colab\n",
        "\n",
        "This notebook runs the entire ML research pipeline on Colab.\n",
        "\n",
        "## üöÄ Quick Start (Running via Cursor ‚Üí Colab Connection)\n",
        "\n",
        "### Step 1: Connect Cursor to Colab\n",
        "1. Press `Ctrl+Shift+P` (or `Cmd+Shift+P` on Mac)\n",
        "2. Type: `Colab: Connect to Colab`\n",
        "3. Authenticate with Google account\n",
        "4. Select a Colab runtime\n",
        "\n",
        "### Step 2: Select Colab Kernel\n",
        "\n",
        "**If kernel selector doesn't work, try these:**\n",
        "\n",
        "**Method A: Command Palette**\n",
        "1. Press `Ctrl+Shift+P`\n",
        "2. Type: `Notebook: Select Notebook Kernel`\n",
        "3. Select \"Colab\" or \"Google Colab\" from list\n",
        "\n",
        "**Method B: Status Bar**\n",
        "1. Click kernel indicator (bottom-right status bar)\n",
        "2. Select \"Select Another Kernel\"\n",
        "3. Choose Colab runtime\n",
        "\n",
        "**Method C: Verify Connection First**\n",
        "1. `Ctrl+Shift+P` ‚Üí `Colab: List Runtimes` (to see if connected)\n",
        "2. If not connected: `Colab: Connect to Colab`\n",
        "3. Then try selecting kernel again\n",
        "\n",
        "**Alternative:** If Cursor connection doesn't work, just run this notebook directly in Colab web interface (see troubleshooting guide)\n",
        "\n",
        "### Step 3: Add API Keys to Colab Secrets\n",
        "**Must be done via Colab web UI:**\n",
        "1. Open https://colab.research.google.com in browser\n",
        "2. Click üîë icon ‚Üí Secrets tab\n",
        "3. Add keys (see cell below for list)\n",
        "\n",
        "### Step 4: Upload Project\n",
        "Upload `ml_research_pipeline` folder to Colab (via web UI or see instructions)\n",
        "\n",
        "### Step 5: Run All Cells\n",
        "- Press `Shift+Enter` through each cell\n",
        "- Or right-click ‚Üí \"Run All\"\n",
        "\n",
        "**Key Names (matching keys.env):**\n",
        "- FINNHUB_API_KEY, NEWS_API_KEY, TIINGO_API_KEY (required)\n",
        "- See cell below for complete list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Quick Diagnostic: Check Colab Connection\n",
        "\n",
        "Run this cell first to verify Colab is accessible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 1] Diagnostic: Check if running on Colab\n",
        "import sys\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COLAB CONNECTION DIAGNOSTIC\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"‚úì SUCCESS: Running on Google Colab!\")\n",
        "    print(f\"‚úì Python executable: {sys.executable}\")\n",
        "    print(f\"‚úì Colab module location: {google.colab.__file__}\")\n",
        "    ON_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"‚úó NOT running on Colab\")\n",
        "    print(f\"  Python executable: {sys.executable}\")\n",
        "    print(\"\\nTo connect:\")\n",
        "    print(\"1. Press Ctrl+Shift+P\")\n",
        "    print(\"2. Type: 'Colab: Connect to Colab'\")\n",
        "    print(\"3. Authenticate and select runtime\")\n",
        "    print(\"4. Then select Colab kernel (top-right or via Command Palette)\")\n",
        "    ON_COLAB = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if ON_COLAB:\n",
        "    print(\"‚úì Ready to proceed - Colab is connected!\")\n",
        "else:\n",
        "    print(\"‚ö† Need to connect to Colab first\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ [STEP 2] Upload Project Files\n",
        "\n",
        "**EASIEST: Upload directly in Cell 5 below - no manual folder upload needed!**\n",
        "\n",
        "**RUN CELL 5** and it will prompt you to upload the zip file. Everything else is automatic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 2] AUTO-SETUP: Create project structure directly in Colab\n",
        "# This will build the entire project in Colab - no manual upload needed!\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"AUTO-SETUP: Creating Project Structure in Colab\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create project directory\n",
        "project_dir = Path('/content/ml_research_pipeline')\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(project_dir)\n",
        "\n",
        "# Create directory structure\n",
        "dirs = [\n",
        "    'src/data', 'src/features', 'src/models', \n",
        "    'src/ensemble', 'src/backtest', 'src/utils',\n",
        "    'data/raw', 'data/processed',\n",
        "    'models/specialists', 'models/meta',\n",
        "    'results', 'artifacts', 'tests', 'notebooks'\n",
        "]\n",
        "\n",
        "for dir_path in dirs:\n",
        "    (project_dir / dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úì Created directory structure\")\n",
        "\n",
        "# Create __init__.py files\n",
        "init_files = [\n",
        "    'src/__init__.py',\n",
        "    'src/data/__init__.py',\n",
        "    'src/features/__init__.py',\n",
        "    'src/models/__init__.py',\n",
        "    'src/ensemble/__init__.py',\n",
        "    'src/backtest/__init__.py',\n",
        "    'src/utils/__init__.py',\n",
        "    'tests/__init__.py'\n",
        "]\n",
        "\n",
        "for init_file in init_files:\n",
        "    (project_dir / init_file).touch()\n",
        "    (project_dir / init_file).write_text('\"\"\"Module initialization.\"\"\"\\n')\n",
        "\n",
        "print(\"‚úì Created __init__.py files\")\n",
        "\n",
        "# Now we'll download the source files from a repository or create them\n",
        "# Since we can't upload directly, we'll use git clone or create files inline\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DOWNLOADING SOURCE CODE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Option 1: If you have the project in a Git repository, uncomment below:\")\n",
        "print(\"  !git clone <your-repo-url> /content/ml_research_pipeline\")\n",
        "print(\"\\nOption 2: We'll create the source files directly (see next cell)\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è IMPORTANT: Cell 5 Upload Issue\n",
        "\n",
        "**If you get \"Upload widget is only available\" error:**\n",
        "\n",
        "This happens because the upload widget only works in Colab web UI, not from Cursor.\n",
        "\n",
        "**Solution:** Run Cell 5 in Colab web browser:\n",
        "1. Open https://colab.research.google.com\n",
        "2. Upload this notebook there\n",
        "3. Run Cell 5 in the browser (it will show file picker)\n",
        "4. Then continue in Cursor or Colab\n",
        "\n",
        "**OR use Method 3 below (manual upload via file browser)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 3 ALTERNATIVE] Manual Upload - Extract if zip already uploaded\n",
        "# Use this if you manually uploaded ml_research_pipeline.zip via Colab file browser\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXTRACTING UPLOADED FILES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check for zip file\n",
        "zip_path = Path('/content/ml_research_pipeline.zip')\n",
        "project_dir = Path('/content/ml_research_pipeline')\n",
        "\n",
        "if zip_path.exists():\n",
        "    print(f\"‚úì Found zip file: {zip_path}\")\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    zip_path.unlink()  # Remove zip after extraction\n",
        "    print(\"‚úì Extracted successfully!\")\n",
        "elif (project_dir / 'src').exists():\n",
        "    print(\"‚úì Project files already exist - no extraction needed\")\n",
        "else:\n",
        "    print(\"‚ö† Zip file not found at /content/ml_research_pipeline.zip\")\n",
        "    print(\"\\nTo upload manually:\")\n",
        "    print(\"1. In Colab web UI, click folder icon (üìÅ) in left sidebar\")\n",
        "    print(\"2. Click 'Upload' button\")\n",
        "    print(\"3. Upload ml_research_pipeline.zip\")\n",
        "    print(\"4. Re-run this cell\")\n",
        "\n",
        "# Verify\n",
        "if (project_dir / 'src').exists():\n",
        "    print(f\"\\n‚úì Project files ready at: {project_dir}\")\n",
        "    print(f\"  Contents: {list((project_dir / 'src').iterdir())}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Project structure not found\")\n",
        "    print(f\"  Current contents: {list(Path('/content').iterdir())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 3] UPLOAD PROJECT FILES\n",
        "# ‚ö†Ô∏è IMPORTANT: This cell has 3 options - choose ONE that works for you\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"UPLOAD PROJECT FILES - CHOOSE ONE METHOD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "project_dir = Path('/content/ml_research_pipeline')\n",
        "\n",
        "# Check if files already exist\n",
        "if (project_dir / 'src').exists():\n",
        "    print(\"‚úì Project files already exist!\")\n",
        "    print(f\"  Location: {project_dir}\")\n",
        "    print(f\"  Contents: {list((project_dir / 'src').iterdir())}\")\n",
        "    print(\"\\n‚úì Skipping upload - files already present\")\n",
        "else:\n",
        "    print(\"\\n‚ö† Project files not found. Choose one method below:\\n\")\n",
        "    \n",
        "    # METHOD 1: Use Colab's file upload (must run in Colab web UI)\n",
        "    print(\"=\"*70)\n",
        "    print(\"METHOD 1: Upload via Colab Web UI (Recommended)\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"1. Open this notebook in Colab web UI: https://colab.research.google.com\")\n",
        "    print(\"2. Run THIS CELL (Cell 5) in the Colab web browser\")\n",
        "    print(\"3. Click 'Choose Files' when prompted\")\n",
        "    print(\"4. Select ml_research_pipeline.zip\")\n",
        "    print(\"5. Wait for upload and extraction\")\n",
        "    print(\"\\nUncomment the code below to use this method:\")\n",
        "    print(\"\"\"\n",
        "    # Uncomment these lines and run in Colab web UI:\n",
        "    # from google.colab import files\n",
        "    # uploaded = files.upload()\n",
        "    # for filename in uploaded.keys():\n",
        "    #     if filename.endswith('.zip'):\n",
        "    #         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    #             zip_ref.extractall('/content/')\n",
        "    #         os.remove(filename)\n",
        "    \"\"\")\n",
        "    \n",
        "    # METHOD 2: Use Google Drive\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"METHOD 2: Upload via Google Drive\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"1. Upload ml_research_pipeline.zip to your Google Drive\")\n",
        "    print(\"2. Uncomment and run the code below:\")\n",
        "    print(\"\"\"\n",
        "    # Uncomment to use Google Drive:\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # # Copy from drive (adjust path to where you uploaded the zip)\n",
        "    # !cp /content/drive/MyDrive/ml_research_pipeline.zip /content/\n",
        "    # !unzip /content/ml_research_pipeline.zip -d /content/\n",
        "    # !rm /content/ml_research_pipeline.zip\n",
        "    \"\"\")\n",
        "    \n",
        "    # METHOD 3: Manual upload via Colab file browser\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"METHOD 3: Manual Upload via Colab File Browser\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"1. In Colab web UI, click folder icon (üìÅ) in left sidebar\")\n",
        "    print(\"2. Click 'Upload' button\")\n",
        "    print(\"3. Upload ml_research_pipeline.zip\")\n",
        "    print(\"4. Then run this code to extract:\")\n",
        "    print(\"\"\"\n",
        "    # Run this after manually uploading zip to /content/:\n",
        "    zip_path = Path('/content/ml_research_pipeline.zip')\n",
        "    if zip_path.exists():\n",
        "        print(\"Found zip file, extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/')\n",
        "        zip_path.unlink()  # Remove zip\n",
        "        print(\"‚úì Extracted successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö† Zip file not found at /content/ml_research_pipeline.zip\")\n",
        "        print(\"  Make sure you uploaded it via the file browser\")\n",
        "    \"\"\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"QUICK FIX: Run this cell in Colab web UI, not from Cursor!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nAfter uploading files, re-run this cell to verify.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install Dependencies\n",
        "%pip install -q pandas numpy scikit-learn xgboost lightgbm yfinance requests python-dotenv tqdm joblib transformers torch sentencepiece\n",
        "print(\"‚úì Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Setup Environment and Keys\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    ON_COLAB = True\n",
        "    print(\"‚úì Running on Google Colab\")\n",
        "except ImportError:\n",
        "    ON_COLAB = False\n",
        "    print(\"‚ö† Not on Colab\")\n",
        "\n",
        "# Set project directory\n",
        "if ON_COLAB:\n",
        "    project_dir = Path('/content/ml_research_pipeline')\n",
        "    if not project_dir.exists():\n",
        "        project_dir = Path('/content')\n",
        "    os.chdir(project_dir)\n",
        "else:\n",
        "    project_dir = Path().absolute()\n",
        "\n",
        "# Setup API keys from Colab secrets - ALL keys from keys.env\n",
        "if ON_COLAB:\n",
        "    from google.colab import userdata\n",
        "    \n",
        "    # All keys from keys.env (matching the actual key names)\n",
        "    keys_from_env = {\n",
        "        'TIINGO_API_KEY': 'b815ff7c64c1a7370b9ae8c0b8907673fdb5eb5f',\n",
        "        'FINAGE_API_KEY': 'API_KEY6aZPLW0IIOEOAZFW1IMW46CC8WIMRP23',\n",
        "        'NEWS_API_KEY': '9ff201f1e68b4544ab5d358a261f1742',\n",
        "        'FINNHUB_API_KEY': 'd28ndhhr01qmp5u9g65gd28ndhhr01qmp5u9g660',\n",
        "        'FINNHUB2_API_KEY': 'd38b891r01qlbdj4nnlgd38b891r01qlbdj4nnm0',\n",
        "        'POLYGON_API_KEY': 'xVilYBLLH5At9uE3r6CIMrusXxWwxp0G',\n",
        "        'TWELVEDATA_API_KEY': '77c34e29fa104ee9bd7834c3b476b824',\n",
        "        'QUANDL_API_KEY': 'fN3R5X9VPSaeqFC6R2hF',\n",
        "        'GROQ_API_KEY': '<GROQ_API_KEY>',\n",
        "        'FRED_API_KEY': '3c86f2f10c5e2b13454447d184ddb268',\n",
        "        'SEC_API_KEY': '0cb9c45a821668958bab90d73e70bc26b28b68ffeb83065da0495d0b7db2c138',\n",
        "        'OPENROUTER_KEY': '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',\n",
        "    }\n",
        "    \n",
        "    print(\"Loading keys from Colab secrets (matching keys.env names)...\")\n",
        "    keys_loaded = 0\n",
        "    for key, default_value in keys_from_env.items():\n",
        "        try:\n",
        "            value = userdata.get(key)\n",
        "            os.environ[key] = value\n",
        "            print(f\"  ‚úì {key}\")\n",
        "            keys_loaded += 1\n",
        "        except:\n",
        "            # Not in secrets, use default from keys.env\n",
        "            os.environ[key] = default_value\n",
        "            print(f\"  ‚ö† {key} not in secrets, using keys.env value\")\n",
        "    \n",
        "    print(f\"\\n‚úì Loaded {keys_loaded}/{len(keys_from_env)} keys from Colab secrets\")\n",
        "    print(\"  ‚Üí Add missing keys via Colab UI: üîë ‚Üí Secrets ‚Üí Add new secret\")\n",
        "else:\n",
        "    # Local: try loading from keys.env\n",
        "    from dotenv import load_dotenv\n",
        "    env_file = project_dir / 'keys.env'\n",
        "    if env_file.exists():\n",
        "        load_dotenv(env_file)\n",
        "        print(\"‚úì Loaded keys from keys.env\")\n",
        "    else:\n",
        "        print(\"‚ö† keys.env not found\")\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(project_dir / 'src'))\n",
        "\n",
        "print(f\"\\nProject directory: {project_dir}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"\\nKey status:\")\n",
        "print(f\"  FINNHUB_API_KEY: {'‚úì' if os.getenv('FINNHUB_API_KEY') else '‚úó'}\")\n",
        "print(f\"  NEWS_API_KEY: {'‚úì' if os.getenv('NEWS_API_KEY') else '‚úó'}\")\n",
        "print(f\"  TIINGO_API_KEY: {'‚úì' if os.getenv('TIINGO_API_KEY') else '‚úó'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 6.5] Fix __init__.py files - Add proper imports\n",
        "# This ensures all modules can be imported correctly\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "project_dir = Path('/content/ml_research_pipeline')\n",
        "src_dir = project_dir / 'src'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FIXING __init__.py FILES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Fix data/__init__.py\n",
        "data_init = src_dir / 'data' / '__init__.py'\n",
        "if data_init.exists():\n",
        "    data_init.write_text('''\"\"\"Data fetching modules.\"\"\"\n",
        "from .price_fetcher import PriceFetcher\n",
        "from .news_fetcher import NewsFetcher\n",
        "__all__ = [\"PriceFetcher\", \"NewsFetcher\"]\n",
        "''')\n",
        "    print(\"‚úì Fixed data/__init__.py\")\n",
        "\n",
        "# Fix features/__init__.py\n",
        "features_init = src_dir / 'features' / '__init__.py'\n",
        "if features_init.exists():\n",
        "    features_init.write_text('''\"\"\"Feature engineering modules.\"\"\"\n",
        "from .feature_builder import FeatureBuilder\n",
        "__all__ = [\"FeatureBuilder\"]\n",
        "''')\n",
        "    print(\"‚úì Fixed features/__init__.py\")\n",
        "\n",
        "# Fix models/__init__.py\n",
        "models_init = src_dir / 'models' / '__init__.py'\n",
        "if models_init.exists():\n",
        "    models_init.write_text('''\"\"\"Model modules.\"\"\"\n",
        "from .base_model import BaseModel, ModelSignal\n",
        "from .xgboost_model import XGBoostModel\n",
        "from .lightgbm_model import LightGBMModel\n",
        "from .sentiment_model import SentimentModel\n",
        "from .rule_based_model import RuleBasedModel\n",
        "__all__ = [\"BaseModel\", \"ModelSignal\", \"XGBoostModel\", \"LightGBMModel\", \"SentimentModel\", \"RuleBasedModel\"]\n",
        "''')\n",
        "    print(\"‚úì Fixed models/__init__.py\")\n",
        "\n",
        "# Fix ensemble/__init__.py\n",
        "ensemble_init = src_dir / 'ensemble' / '__init__.py'\n",
        "if ensemble_init.exists():\n",
        "    ensemble_init.write_text('''\"\"\"Ensemble modules.\"\"\"\n",
        "from .meta_ensemble import MetaEnsemble\n",
        "__all__ = [\"MetaEnsemble\"]\n",
        "''')\n",
        "    print(\"‚úì Fixed ensemble/__init__.py\")\n",
        "\n",
        "# Fix backtest/__init__.py\n",
        "backtest_init = src_dir / 'backtest' / '__init__.py'\n",
        "if backtest_init.exists():\n",
        "    backtest_init.write_text('''\"\"\"Backtesting modules.\"\"\"\n",
        "from .walkforward_backtest import WalkForwardBacktest\n",
        "__all__ = [\"WalkForwardBacktest\"]\n",
        "''')\n",
        "    print(\"‚úì Fixed backtest/__init__.py\")\n",
        "\n",
        "# Verify source files exist\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFYING SOURCE FILES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "required_files = [\n",
        "    'src/data/price_fetcher.py',\n",
        "    'src/data/news_fetcher.py',\n",
        "    'src/features/feature_builder.py',\n",
        "    'src/models/base_model.py',\n",
        "    'src/models/xgboost_model.py',\n",
        "    'src/models/lightgbm_model.py',\n",
        "    'src/models/sentiment_model.py',\n",
        "    'src/models/rule_based_model.py',\n",
        "    'src/ensemble/meta_ensemble.py',\n",
        "    'src/backtest/walkforward_backtest.py',\n",
        "    'src/utils/config.py',\n",
        "    'src/utils/helpers.py',\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file_path in required_files:\n",
        "    full_path = project_dir / file_path\n",
        "    if full_path.exists():\n",
        "        print(f\"  ‚úì {file_path}\")\n",
        "    else:\n",
        "        print(f\"  ‚úó {file_path} - MISSING!\")\n",
        "        missing.append(file_path)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n‚ö† {len(missing)} files are missing!\")\n",
        "    print(\"  ‚Üí Make sure you uploaded the complete project folder\")\n",
        "    print(\"  ‚Üí Check that ml_research_pipeline.zip was fully extracted\")\n",
        "else:\n",
        "    print(f\"\\n‚úì All {len(required_files)} required files found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Add All Keys to Colab Secrets\n",
        "\n",
        "Run this cell to see which keys need to be added to Colab secrets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check and add keys to Colab secrets\n",
        "# This will show you which keys are missing and need to be added via UI\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    \n",
        "    # All keys from keys.env\n",
        "    keys_from_env = {\n",
        "        \"TIINGO_API_KEY\": \"b815ff7c64c1a7370b9ae8c0b8907673fdb5eb5f\",\n",
        "        \"FINAGE_API_KEY\": \"API_KEY6aZPLW0IIOEOAZFW1IMW46CC8WIMRP23\",\n",
        "        \"NEWS_API_KEY\": \"9ff201f1e68b4544ab5d358a261f1742\",\n",
        "        \"FINNHUB_API_KEY\": \"d28ndhhr01qmp5u9g65gd28ndhhr01qmp5u9g660\",\n",
        "        \"FINNHUB2_API_KEY\": \"d38b891r01qlbdj4nnlgd38b891r01qlbdj4nnm0\",\n",
        "        \"POLYGON_API_KEY\": \"xVilYBLLH5At9uE3r6CIMrusXxWwxp0G\",\n",
        "        \"TWELVEDATA_API_KEY\": \"77c34e29fa104ee9bd7834c3b476b824\",\n",
        "        \"QUANDL_API_KEY\": \"fN3R5X9VPSaeqFC6R2hF\",\n",
        "        \"GROQ_API_KEY\": \"<GROQ_API_KEY>\",\n",
        "        \"FRED_API_KEY\": \"3c86f2f10c5e2b13454447d184ddb268\",\n",
        "        \"SEC_API_KEY\": \"0cb9c45a821668958bab90d73e70bc26b28b68ffeb83065da0495d0b7db2c138\",\n",
        "        \"OPENROUTER_KEY\": \"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\",\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"COLAB SECRETS STATUS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    existing = []\n",
        "    missing = []\n",
        "    \n",
        "    for key, value in keys_from_env.items():\n",
        "        try:\n",
        "            userdata.get(key)\n",
        "            existing.append(key)\n",
        "            print(f\"‚úì {key} - Already in secrets\")\n",
        "        except:\n",
        "            missing.append((key, value))\n",
        "            print(f\"‚úó {key} - Missing from secrets\")\n",
        "    \n",
        "    if missing:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ADD THESE KEYS TO COLAB SECRETS\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\n1. Click üîë icon (left sidebar)\")\n",
        "        print(\"2. Go to 'Secrets' tab\")\n",
        "        print(\"3. Click 'Add new secret' for each:\\n\")\n",
        "        \n",
        "        for key, value in missing:\n",
        "            print(f\"Key Name: {key}\")\n",
        "            print(f\"Value: {value}\")\n",
        "            print()\n",
        "        \n",
        "        print(\"After adding, re-run Step 2 cell to load them.\")\n",
        "    else:\n",
        "        print(\"\\n‚úì All keys are in Colab secrets!\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"Not running on Colab - keys will be loaded from keys.env file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Setup Project Structure\n",
        "dirs = ['data/raw', 'data/processed', 'models/specialists', 'models/meta', 'results', 'artifacts']\n",
        "for dir_path in dirs:\n",
        "    (project_dir / dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úì Project structure ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Pipeline - Data Fetching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and run data fetching\n",
        "from data import PriceFetcher, NewsFetcher\n",
        "import pandas as pd\n",
        "from utils.config import PROCESSED_DATA_DIR\n",
        "\n",
        "# Configuration\n",
        "TICKER = \"AAPL\"\n",
        "START_DATE = \"2020-01-01\"\n",
        "END_DATE = \"2023-12-31\"\n",
        "INDEX_SYMBOL = \"^GSPC\"\n",
        "\n",
        "print(f\"Fetching data for {TICKER}...\")\n",
        "\n",
        "# Fetch prices\n",
        "price_fetcher = PriceFetcher()\n",
        "stock_prices = price_fetcher.fetch(TICKER, START_DATE, END_DATE)\n",
        "print(f\"‚úì Fetched {len(stock_prices)} days of price data\")\n",
        "\n",
        "# Fetch index\n",
        "index_prices = price_fetcher.fetch_index(INDEX_SYMBOL, START_DATE, END_DATE)\n",
        "print(f\"‚úì Fetched {len(index_prices)} days of index data\")\n",
        "\n",
        "# Fetch news\n",
        "news_fetcher = NewsFetcher()\n",
        "news_data = news_fetcher.fetch_all(TICKER, START_DATE, END_DATE)\n",
        "print(f\"‚úì Fetched {len(news_data)} news articles\")\n",
        "\n",
        "# Save\n",
        "stock_prices.to_csv(PROCESSED_DATA_DIR / f\"{TICKER}_prices.csv\")\n",
        "index_prices.to_csv(PROCESSED_DATA_DIR / f\"{INDEX_SYMBOL.replace('^', '')}_prices.csv\")\n",
        "if not news_data.empty:\n",
        "    news_data.to_csv(PROCESSED_DATA_DIR / f\"{TICKER}_news.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úì Data fetching complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from features import FeatureBuilder\n",
        "from utils.helpers import save_artifact\n",
        "\n",
        "# Load data\n",
        "stock_prices = pd.read_csv(PROCESSED_DATA_DIR / f\"{TICKER}_prices.csv\", index_col=0, parse_dates=True)\n",
        "index_prices = pd.read_csv(PROCESSED_DATA_DIR / f\"{INDEX_SYMBOL.replace('^', '')}_prices.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "news_file = PROCESSED_DATA_DIR / f\"{TICKER}_news.csv\"\n",
        "if news_file.exists():\n",
        "    news_data = pd.read_csv(news_file, parse_dates=['date'])\n",
        "else:\n",
        "    news_data = pd.DataFrame()\n",
        "\n",
        "# Build features\n",
        "builder = FeatureBuilder()\n",
        "features = builder.build_all_features(stock_prices, index_prices, news_data)\n",
        "\n",
        "print(f\"‚úì Built {len(features.columns)} features, {len(features)} samples\")\n",
        "\n",
        "# Save\n",
        "features.to_csv(PROCESSED_DATA_DIR / f\"{TICKER}_features.csv\")\n",
        "save_artifact(builder.feature_metadata, PROCESSED_DATA_DIR / f\"{TICKER}_feature_metadata.pkl\")\n",
        "\n",
        "print(\"‚úì Feature engineering complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train Price Models (XGBoost + LightGBM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 9] Train Price Models\n",
        "from models import XGBoostModel, LightGBMModel\n",
        "from utils.config import SPECIALIST_MODELS_DIR\n",
        "\n",
        "# Load features\n",
        "features = pd.read_csv(PROCESSED_DATA_DIR / f\"{TICKER}_features.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "feature_cols = [col for col in features.columns \n",
        "                if col not in ['target_return_1d', 'target_direction', 'open', 'high', 'low', 'close', 'volume']]\n",
        "X = features[feature_cols].fillna(0)\n",
        "y = features['target_direction']\n",
        "mask = y != 0\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print(f\"Training on {len(X)} samples\")\n",
        "\n",
        "# Train XGBoost\n",
        "xgb = XGBoostModel(min_confidence=0.6)\n",
        "xgb_metrics = xgb.train(X, y)\n",
        "print(f\"‚úì XGBoost: {xgb_metrics}\")\n",
        "xgb.save(SPECIALIST_MODELS_DIR / f\"{TICKER}_xgb.model\")\n",
        "\n",
        "# Train LightGBM\n",
        "lgb = LightGBMModel(min_confidence=0.6)\n",
        "lgb_metrics = lgb.train(X, y)\n",
        "print(f\"‚úì LightGBM: {lgb_metrics}\")\n",
        "lgb.save(SPECIALIST_MODELS_DIR / f\"{TICKER}_lgb.txt\")\n",
        "\n",
        "print(\"‚úì Price models trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Train Sentiment Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 10] Train Sentiment Models\n",
        "from models import SentimentModel, RuleBasedModel\n",
        "\n",
        "# Train Sentiment\n",
        "sentiment = SentimentModel(min_confidence=0.6, use_pretrained=False)\n",
        "sentiment_metrics = sentiment.train(X, y)\n",
        "print(f\"‚úì Sentiment: {sentiment_metrics}\")\n",
        "sentiment.save(SPECIALIST_MODELS_DIR / f\"{TICKER}_sentiment.pkl\")\n",
        "\n",
        "# Train Rule-based\n",
        "rule = RuleBasedModel(min_confidence=0.6)\n",
        "rule_metrics = rule.train(X, y)\n",
        "print(f\"‚úì Rule-based: {rule_metrics}\")\n",
        "rule.save(SPECIALIST_MODELS_DIR / f\"{TICKER}_rule.pkl\")\n",
        "\n",
        "print(\"‚úì Sentiment models trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Train Meta-Ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 11] Train Meta-Ensemble\n",
        "from ensemble import MetaEnsemble\n",
        "from utils.config import META_MODEL_DIR\n",
        "\n",
        "# Load all specialists\n",
        "xgb = XGBoostModel()\n",
        "xgb.load(SPECIALIST_MODELS_DIR / f\"{TICKER}_xgb.model\")\n",
        "\n",
        "lgb = LightGBMModel()\n",
        "lgb.load(SPECIALIST_MODELS_DIR / f\"{TICKER}_lgb.txt\")\n",
        "\n",
        "sentiment = SentimentModel()\n",
        "sentiment.load(SPECIALIST_MODELS_DIR / f\"{TICKER}_sentiment.pkl\")\n",
        "\n",
        "rule = RuleBasedModel()\n",
        "rule.load(SPECIALIST_MODELS_DIR / f\"{TICKER}_rule.pkl\")\n",
        "\n",
        "specialists = [xgb, lgb, sentiment, rule]\n",
        "\n",
        "# Extract market features\n",
        "vol_cols = [col for col in X.columns if 'volatility' in col.lower()]\n",
        "news_cols = [col for col in X.columns if 'news' in col.lower()]\n",
        "\n",
        "market_features = pd.DataFrame(index=X.index)\n",
        "if vol_cols:\n",
        "    market_features['volatility'] = X[vol_cols[0]]\n",
        "if news_cols:\n",
        "    market_features['news_intensity'] = X[news_cols[0]]\n",
        "market_features = market_features.fillna(0)\n",
        "\n",
        "# Train ensemble\n",
        "ensemble = MetaEnsemble(specialists, min_confidence=0.6)\n",
        "meta_metrics = ensemble.train(X, y, market_features)\n",
        "print(f\"‚úì Meta-ensemble: {meta_metrics}\")\n",
        "ensemble.save(META_MODEL_DIR / f\"{TICKER}_meta_ensemble.pkl\")\n",
        "\n",
        "print(\"‚úì Meta-ensemble trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Walk-Forward Backtest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [STEP 12] Walk-Forward Backtest\n",
        "from backtest import WalkForwardBacktest\n",
        "from utils.config import RESULTS_DIR\n",
        "\n",
        "# Load data\n",
        "features = pd.read_csv(PROCESSED_DATA_DIR / f\"{TICKER}_features.csv\", index_col=0, parse_dates=True)\n",
        "prices = pd.read_csv(PROCESSED_DATA_DIR / f\"{TICKER}_prices.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "feature_cols = [col for col in features.columns \n",
        "                if col not in ['target_return_1d', 'target_direction', 'open', 'high', 'low', 'close', 'volume']]\n",
        "X = features[feature_cols].fillna(0)\n",
        "y = features['target_return_1d']\n",
        "\n",
        "# Load ensemble\n",
        "ensemble = MetaEnsemble(specialists)\n",
        "ensemble.load(META_MODEL_DIR / f\"{TICKER}_meta_ensemble.pkl\")\n",
        "\n",
        "# Run backtest\n",
        "backtest = WalkForwardBacktest(ensemble, train_window_days=252, test_window_days=21)\n",
        "results = backtest.run(X, prices, y)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BACKTEST RESULTS\")\n",
        "print(\"=\"*60)\n",
        "for key, value in results['metrics'].items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Save\n",
        "backtest.save_results(RESULTS_DIR / f\"{TICKER}_backtest_results.pkl\")\n",
        "results['predictions'].to_csv(RESULTS_DIR / f\"{TICKER}_predictions.csv\")\n",
        "results['actuals'].to_csv(RESULTS_DIR / f\"{TICKER}_actuals.csv\")\n",
        "\n",
        "print(\"\\n‚úì Backtest complete - results saved to results/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
